{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "* WikiText2 dataset contains wiki.train.tokens, wiki.valid.tokens, and wiki.test.tokens. No processing is needed other than replacing newlines with ``<eos>`` tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "import gc\n",
    "import struct\n",
    "\n",
    "import math\n",
    "import copy\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def seed_everything(val):\n",
    "    \"\"\" For reproducibility - Seeds all relevant random generators to the same value. \"\"\"\n",
    "    random.seed(val)\n",
    "    os.environ['PYTHONHASHSEED'] = str(val)\n",
    "    np.random.seed(val)\n",
    "    torch.manual_seed(val)\n",
    "    torch.cuda.manual_seed(val)\n",
    "    torch.cuda.manual_seed_all(val)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    print('Manual seed changed successfully.')\n",
    "\n",
    "\"\"\" DO NOT CHANGE THE SEED! \"\"\"\n",
    "seed = 42\n",
    "seed_everything(seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Multi_Corpus_loader:\n",
    "# The Multi Corpus loader class loads all of the files to create a large \n",
    "# uniform dictionary with all of the words in the data files,Â in case \n",
    "# some words are missing from the test or validation datasets or if the\n",
    "# train data set does not contain all of the words. This loader is not used in this model.\n",
    "    def __init__(self, directory, files, max_seq):\n",
    "        self.load_texts(directory, files)\n",
    "        \n",
    "        self.dictionary = {}  \n",
    "        self.tokenize()\n",
    "\n",
    "        self.flat_train = self.FlatTensor(self.train)\n",
    "        self.flat_test  = self.FlatTensor(self.test )\n",
    "        self.flat_valid = self.FlatTensor(self.valid)\n",
    "\n",
    "        self.sequences_train = self.sequences_splitter(self.flat_train, max_seq)\n",
    "        self.sequences_test  = self.sequences_splitter(self.flat_test , max_seq)\n",
    "        self.sequences_valid = self.sequences_splitter(self.flat_valid, max_seq)\n",
    "\n",
    "    def load_texts(self, directory, files):\n",
    "        tokens = []\n",
    "        for idx, file in enumerate(files):\n",
    "            with open(os.path.join(directory,file), 'r', encoding=\"utf8\") as f:\n",
    "\n",
    "                # Set train dataset raw test into flat tensor\n",
    "                if 'train' in file:\n",
    "                    f.seek(0)\n",
    "                    self.train = f.read().replace('\\n','<eos>')                         \n",
    "                    # Add new words to dictionary\n",
    "                    tokens += self.train.strip().split()\n",
    "\n",
    "                # Set test dataset raw test into flat tensor\n",
    "                if 'test' in file:\n",
    "                    f.seek(0)\n",
    "                    self.test  = f.read().replace('\\n','<eos>')\n",
    "                    # Add new words to dictionary\n",
    "                    tokens += self.train.strip().split()\n",
    "\n",
    "                # Set validation dataset raw test into flat tensor\n",
    "                if 'valid' in file:\n",
    "                    f.seek(0)\n",
    "                    self.valid = f.read().replace('\\n','<eos>')\n",
    "                    # Add new words to dictionary\n",
    "                    tokens += self.train.strip().split()\n",
    "\n",
    "            f.close()\n",
    "\n",
    "        self.tokens = np.unique(tokens) \n",
    "\n",
    "    def tokenize(self):\n",
    "        for i, token in enumerate(self.tokens):\n",
    "            if token not in self.dictionary:\n",
    "                # Create a dictionary mapping unique numbers to words\n",
    "                self.dictionary[token] = len(self.dictionary) \n",
    "\n",
    "    def FlatTensor(self, text):\n",
    "        tokens = text.strip().split() # Tokenize the raw text \n",
    "        # Flat tensor of the tokenized text\n",
    "        flat_tensor = torch.tensor([self.dictionary[token] for token in tokens], dtype=torch.long)\n",
    "        return flat_tensor\n",
    "\n",
    "    def sequences_splitter(self, flat_tensor, sequence_length=64):\n",
    "        # Divide the flat tensor into a list of sequences, removing extra elements\n",
    "        num_sequences  = len(flat_tensor) // sequence_length\n",
    "        sequences = torch.stack([flat_tensor[i*sequence_length:(i+1)*sequence_length] for i in range(num_sequences)])\n",
    "        return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus_loader:\n",
    "  def __init__(self, directory, file, max_seq, dictionary=None):\n",
    "    '''\n",
    "    Input: \n",
    "        directory       Directory path for dataset\n",
    "        file            Dataset file in the directory\n",
    "        max_seq         Maximum sequence length\n",
    "        dictionary      Loads existing dictionary\n",
    "\n",
    "\n",
    "    load_raw_text function:\n",
    "        This function loads the raw data and replaces newline with <eos>\n",
    "    \n",
    "    tokenize function:\n",
    "        This function tokenize the input data from the file and creates a dictionary.\n",
    "        for each word, sign, etc., there is a unique token according to its location in\n",
    "        the dictionary. In addition, the function creates a flat tensor of the input data\n",
    "        with tokens.\n",
    "\n",
    "    sequences_splitter function:\n",
    "        This function creates sequences in set sequence length.\n",
    "    '''\n",
    "    if dictionary == None:\n",
    "      self.dictionary = {}\n",
    "    else:\n",
    "      self.dictionary = dictionary\n",
    "      \n",
    "    self.sequences  = []\n",
    "    \n",
    "    self.load_raw_text(directory, file)\n",
    "    self.tokenize()\n",
    "    self.sequences_splitter(max_seq)\n",
    "    \n",
    "  def load_raw_text(self, directory, file):\n",
    "    # Load the raw text file and store as a string\n",
    "    with open(os.path.join(directory, file), 'r', encoding=\"utf8\") as f:     \n",
    "      self.text = f.read().replace('\\n','<eos>') # Replace newline with <eos>\n",
    "      f.close() # Reset counter \n",
    "   \n",
    "  def tokenize(self):\n",
    "    tokens = self.text.strip().split() # Tokenize the raw text \n",
    "\n",
    "    # In case there is a dictionary already\n",
    "    if len(self.dictionary) > 0:\n",
    "      for i, token in enumerate(tokens):\n",
    "        if token not in self.dictionary:\n",
    "          # replaces words that are not in thge dictionary to the unknown token\n",
    "          tokens[i] = '<unk>'\n",
    "\n",
    "    # Creating new dictionary\n",
    "    else:\n",
    "      for i, token in enumerate(tokens):\n",
    "        if token not in self.dictionary:\n",
    "          # Create a dictionary mapping unique numbers to words\n",
    "          self.dictionary[token] = len(self.dictionary) \n",
    "\n",
    "    # Flat tensor of the tokenized text\n",
    "    self.flat_tensor = torch.tensor([self.dictionary[token] for token in tokens], dtype=torch.long)\n",
    "    \n",
    "  def sequences_splitter(self, sequence_length=64):\n",
    "    # Divide the flat tensor into a tensor of sequences tensors (batch the text to sequence_length batches)\n",
    "    num_sequences  = self.flat_tensor.size(0) // sequence_length\n",
    "    self.sequences = torch.stack([self.flat_tensor[i*sequence_length:(i+1)*sequence_length] for i in range(num_sequences)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding & positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embed_dict(nn.Module):\n",
    "    def __init__(self, dictionary_size, d_model):\n",
    "        super().__init__()\n",
    "        '''\n",
    "        Input: \n",
    "            d_model             Dimensions of the input\n",
    "            dictionary_size     Dictionary size\n",
    "\n",
    "\n",
    "        forward function:\n",
    "            This function creates embedding matrix from the input tensor.\n",
    "        '''\n",
    "        # Creating embedding for input's dictionary\n",
    "        self.embed = nn.Embedding(dictionary_size, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.embed(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Positional_Embedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=64, dropout=0.1):\n",
    "        super().__init__()\n",
    "        '''\n",
    "        Input: \n",
    "            d_model         Dimensions of the input\n",
    "            max_len         Maximum sequence length\n",
    "            dropout         Percentage of dropout\n",
    "\n",
    "        positional_encoding function:\n",
    "            This function calculates and adds the positional encoding to the input embedding in the forward function\n",
    "\n",
    "        forward function:\n",
    "            This function adds positional embedding to the input embedding \n",
    "        '''\n",
    "        # Dropout layer\n",
    "        self.dropout  = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Positional embedding vector\n",
    "        pos_embedding = self.positional_encoding(d_model, max_len)\n",
    "        self.register_buffer('pos_embedding', pos_embedding) # Registering the parameter as buffer so it won't be trained by the optimizer and remain constant troughout the training process.\n",
    "\n",
    "    @staticmethod  # Static method are accessible directly from an API object's constructor, while not require to create an object of the class to access the function (example = Positional_Embedding.positional_encoding(d_model, seq_length))\n",
    "    def positional_encoding(d_model, seq_length):\n",
    "        # Preallocation positional vector\n",
    "        pos_encoding = torch.zeros(seq_length, d_model)\n",
    "\n",
    "        for pos in range(seq_length):\n",
    "            for i in range(0, d_model//2):\n",
    "                pos_encoding[pos, 2*i]     = torch.sin(torch.tensor(pos / (10000 ** (2*i / d_model))))\n",
    "                pos_encoding[pos, 2*i + 1] = torch.cos(torch.tensor(pos / (10000 ** (2*i / d_model))))\n",
    "        \n",
    "        return pos_encoding\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add the positions tensor to the input tensor \n",
    "        if x.ndim == 2:\n",
    "            return self.dropout(x + self.pos_embedding[:x.size(0)])\n",
    "        else:\n",
    "            return self.dropout(x + self.pos_embedding[:x.size(1)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clones(module, N):\n",
    "    # generate multiple layers\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(size):\n",
    "    # Returns the upper triangular part of a matrix (2-D tensor) or batch of matrices input, the other elements of the result tensor out are set to 0.\n",
    "    return torch.triu(torch.ones(size, size) * float('-inf'), diagonal=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_mask(sequence_length, batch_size, index):\n",
    "    # Sequence mask for the encoder \n",
    "    mask = torch.zeros(batch_size, sequence_length)\n",
    "    mask[:,index] = 1\n",
    "    mask = mask.bool()\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch transformer hyperparameters optimizer https://github.com/dmlc/dgl/blob/master/examples/pytorch/transformer/optims/noamopt.py\n",
    "class NoamOpt(object):\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        \"\"\"\n",
    "        model_size: hidden size\n",
    "        factor: coefficient\n",
    "        warmup: warm up steps(step ** (-0.5) == step * warmup ** (-1.5) holds when warmup equals step)\n",
    "        \"\"\"\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "\n",
    "    def rate(self, step=None):\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * (\n",
    "            self.model_size ** (-0.5)\n",
    "            * min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
    "\n",
    "    def step(self):\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p[\"lr\"] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer layers & blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, output_dim, hidden_dim, num_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        '''\n",
    "        Input: \n",
    "            d_model         Dimensions of the input\n",
    "            output_dim      Dimensions of the output for feedforward block\n",
    "            hidden_dim      Dimensions of the inner layers (i.e., the first fully connected in Feed Forward block)\n",
    "            num_heads       Number of heads for MultiheadAttention block (default=8)\n",
    "            dropout         Percentage of dropout\n",
    "\n",
    "        forward function:\n",
    "            This function run a encoder block in a sequential manner according to transformer architecture\n",
    "            and gives an output.\n",
    "        '''\n",
    "        # Transformer block dimensions\n",
    "        self.input_dim  = d_model\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim \n",
    "        self.num_heads  = num_heads\n",
    "\n",
    "        #==== ENCODER BLOCK LAYERS ====# \n",
    "        # Multi-Head Attention block\n",
    "        self.attention   = nn.MultiheadAttention(d_model, num_heads, dropout=dropout, batch_first=True)\n",
    "       \n",
    "        # Feed Forward block\n",
    "        self.feedforward = nn.Sequential(nn.Linear(d_model, hidden_dim),     # Fully connected\n",
    "                                         nn.ReLU(),                          # Activation\n",
    "                                         nn.Linear(hidden_dim, output_dim))  # Fully connected\n",
    "        # Add & Norm block\n",
    "        self.attention_norm  = nn.LayerNorm(d_model)\n",
    "        self.output_norm     = nn.LayerNorm(output_dim)\n",
    "        \n",
    "        # Dropout layer \n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, input, mask=None):\n",
    "        # Gets input through self attention layer\n",
    "        attention_output   = self.attention(input, input, input, key_padding_mask=mask)[0] # Takes only output (without weights)\n",
    "        \n",
    "        # Normalize the output from the attention layer\n",
    "        attention_output   = self.attention_norm(input + self.dropout1(attention_output))\n",
    "        \n",
    "        # Feedforward layer \n",
    "        feedforward_output = self.feedforward(attention_output)    \n",
    "        \n",
    "        # Encoder layer's output with normalization\n",
    "        output = self.output_norm(attention_output + self.dropout2(feedforward_output))\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, output_dim, hidden_dim, num_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        '''\n",
    "        Input: \n",
    "            d_model         Dimensions of the input\n",
    "            output_dim      Dimensions of the output for feedforward block\n",
    "            hidden_dim      Dimensions of the inner layers (i.e., fully connected in Feed Forward block)\n",
    "            num_heads       Number of heads for MultiheadAttention block (default=8)\n",
    "            dropout         Percentage of dropout\n",
    "\n",
    "        forward function:\n",
    "            This function run a Decoder block in a sequential manner according to transformer architecture\n",
    "            and gives an output.\n",
    "        '''\n",
    "        # Transformer block dimensions\n",
    "        self.input_dim  = d_model\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim \n",
    "        self.num_heads  = num_heads\n",
    "\n",
    "        #==== DECODER BLOCK LAYERS ====# \n",
    "        # Multi-Head Attention block\n",
    "        self.masked_attention  = nn.MultiheadAttention(d_model, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.encoder_attention = nn.MultiheadAttention(d_model, num_heads, dropout=dropout, batch_first=True)\n",
    "\n",
    "        # Feed Forward block\n",
    "        self.feedforward = nn.Sequential(nn.Linear(d_model, hidden_dim),     # Fully connected\n",
    "                                         nn.ReLU(),                          # Activation\n",
    "                                         nn.Linear(hidden_dim, output_dim))  # Fully connected\n",
    "        \n",
    "        # Add & Norm block\n",
    "        self.masked_attention_norm  = nn.LayerNorm(d_model)\n",
    "        self.encoder_attention_norm = nn.LayerNorm(d_model)\n",
    "        self.output_norm            = nn.LayerNorm(output_dim)\n",
    "        \n",
    "        # Dropout layer \n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, input, encoder_input, sequence_mask, target_mask):\n",
    "        # Gets the input to the decoder layer through masked Multi-head attention layer\n",
    "        masked_attention_output  = self.masked_attention(input, input, input, attn_mask=target_mask)[0]              # Takes only output (without weights)\n",
    "        # Normalize and add the input to the output of the masked Multi-head attention layer\n",
    "        masked_attention_output  = self.masked_attention_norm(input + self.dropout1(masked_attention_output))\n",
    "        \n",
    "        \n",
    "        # Gets the input to the decoder layer and the output of the encoder through Multi-head attention layer\n",
    "        encoder_attention_output = self.encoder_attention(masked_attention_output, encoder_input, encoder_input)[0]  # Takes only output (without weights)\n",
    "        # Normalize and add the input to the output of the second masked Multi-head attention layer\n",
    "        encoder_attention_output = self.encoder_attention_norm(masked_attention_output + self.dropout2(encoder_attention_output))\n",
    "\n",
    "        \n",
    "        # Feedforward layer \n",
    "        feedforward_output = self.feedforward(encoder_attention_output)       \n",
    "        # Decoder layer's output with normalization\n",
    "        output = self.output_norm(encoder_attention_output + self.dropout3(feedforward_output))\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    # This class defines the Encoder of the transformer. Since the Encoder is composed\n",
    "    # from several Encoder blocks, this class calls for N number of blocks and replaces\n",
    "    # the input tensor with the output of the previous block in the chain, so each block\n",
    "    # process the data given by the previous block and not the initial input tensor.\n",
    "\n",
    "    def __init__(self, dictionary_size, d_model, output_dim, hidden_dim, max_seq, N, heads):\n",
    "        super().__init__()\n",
    "        '''\n",
    "        Input: \n",
    "            dictionary_size     Dictionary size\n",
    "            d_model             Dimensions of the input\n",
    "            output_dim          Dimensions of the output for feedforward block\n",
    "            hidden_dim          Dimensions of the inner layers (i.e., fully connected in Feed Forward block)\n",
    "            max_seq             Maximum sequence length\n",
    "            N                   Number of blocks\n",
    "            heads               Number of heads for MultiheadAttention block (default=8)\n",
    "\n",
    "        forward function:\n",
    "            This function set the ENCODER in a sequential manner with Nx blocks according to transformer architecture\n",
    "            and gives an output.\n",
    "        '''\n",
    "        self.N         = N                                      # Number of Encoder blocks\n",
    "        self.embed     = Embed_dict(dictionary_size, d_model)   # embedded input\n",
    "        self.pos_embed = Positional_Embedding(d_model, max_seq) # Positional embedding (added to embedded input)\n",
    "        self.norm      = nn.LayerNorm(d_model)                  # Normalize layer\n",
    "        \n",
    "        # Multiply number of blocks Nx times\n",
    "        self.layers = get_clones(EncoderBlock(d_model, output_dim, hidden_dim, heads), N)\n",
    "\n",
    "    def forward(self, input, sequence_mask):\n",
    "        # Create embedding of the input data and adds positional encoding\n",
    "        x = self.embed(input)\n",
    "        x = self.pos_embed(x)\n",
    "\n",
    "        for i in range(self.N):\n",
    "            # Replaces the input tensor with the output of the previous block\n",
    "            x = self.layers[i](x, sequence_mask)\n",
    "        \n",
    "        return self.norm(x)\n",
    "  \n",
    "\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    # This class defines the Decoder of the transformer. Since the Decoder is composed\n",
    "    # from several Decoder blocks, this class calls for N number of blocks and replaces\n",
    "    # the input tensor with the output of the previous block in the chain, so each block\n",
    "    # process the data given by the previous block and not the initial input tensor.\n",
    "    \n",
    "    def __init__(self, dictionary_size, d_model, output_dim, hidden_dim, max_seq, N, heads):\n",
    "        super().__init__()\n",
    "        '''\n",
    "        Input: \n",
    "            dictionary_size     Dictionary size\n",
    "            d_model             Dimensions of the input\n",
    "            output_dim          Dimensions of the output for feedforward block\n",
    "            hidden_dim          Dimensions of the inner layers (i.e., fully connected in Feed Forward block)\n",
    "            max_seq             Maximum sequence length\n",
    "            N                   Number of blocks\n",
    "            heads               Number of heads for MultiheadAttention block (default=8)\n",
    "\n",
    "        forward function:\n",
    "            This function set the DECODER in a sequential manner with Nx blocks according to transformer architecture\n",
    "            and gives an output.\n",
    "        '''\n",
    "        self.N         = N                                      # Number of Encoder blocks\n",
    "        self.embed     = Embed_dict(dictionary_size, d_model)   # embedded input\n",
    "        self.pos_embed = Positional_Embedding(d_model, max_seq) # Positional embedding (added to embedded input)\n",
    "        self.norm      = nn.LayerNorm(d_model)                  # Normalize layer\n",
    "        \n",
    "        # Multiply number of blocks Nx times\n",
    "        self.layers = get_clones(DecoderBlock(d_model, output_dim, hidden_dim, heads), N)\n",
    "\n",
    "    def forward(self, input, encoder_outputs, sequence_mask, target_mask):\n",
    "        # Create embedding of the input data and adds positional encoding\n",
    "        x = self.embed(input)\n",
    "        x = self.pos_embed(x)\n",
    " \n",
    "        for i in range(self.N):\n",
    "            # Replaces the input tensor with the output of the previous block           \n",
    "            x = self.layers[i](x, encoder_outputs, sequence_mask, target_mask)\n",
    "        \n",
    "        return self.norm(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dictionary_size, d_model, output_dim, hidden_dim, max_seq, N, heads):\n",
    "        super().__init__()\n",
    "        '''\n",
    "        Input: \n",
    "            dictionary_size     dictionary size\n",
    "            d_model             dimensions of the input\n",
    "            output_dim          dimensions of the output for feedforward block\n",
    "            hidden_dim          dimensions of the inner layers (i.e., fully connected in Feed Forward block)\n",
    "            max_seq             Maximum sequence length\n",
    "            N                   Number of blocks\n",
    "            heads               number of heads for MultiheadAttention block (default=8)\n",
    "\n",
    "        forward function:\n",
    "            This function set the TRANSFORMER in a sequential manner with with linear layer before output. \n",
    "            In contrast to the transformer architecture, the class do not include softmax layer, since it is included in the\n",
    "            loss function.\n",
    "        '''\n",
    "        self.encoder = Encoder(dictionary_size, d_model, output_dim, hidden_dim, max_seq, N, heads)\n",
    "        self.decoder = Decoder(dictionary_size, d_model, output_dim, hidden_dim, max_seq, N, heads)\n",
    "        self.linear  = nn.Linear(d_model, dictionary_size)\n",
    "\n",
    "    def forward(self, src, trg, src_mask, trg_mask):\n",
    "        encoder_output = self.encoder(src, src_mask)\n",
    "        decoder_output = self.decoder(trg, encoder_output, src_mask, trg_mask)\n",
    "        output         = self.linear(decoder_output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Evaluation, Prediction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, seq_len, epoch, scheduler, optimizer, criterion, device, chosen_sched):\n",
    "    '''\n",
    "    Input: \n",
    "        model               The transformer model\n",
    "        train_loader        Data loader of the train sequences in batches\n",
    "        seq_len             Maximum sequence length\n",
    "        epoch               Number of current epoch\n",
    "        scheduler           Learning rate scheduler \n",
    "        optimizer           Type of optimizer for model's weights\n",
    "        Criterion           Loss criterion\n",
    "        device              Computation device\n",
    "        chosen_sched        Name of the chosen scheduler\n",
    "\n",
    "    Output:\n",
    "        ppl_train_monitor   List of model's training performance - include: ppl, batch number, epoch, learning rate\n",
    "    '''\n",
    "    ppl_train_monitor = []\n",
    "    gc.collect()\n",
    "    model.train()            # turn on train mode\n",
    "    total_loss = 0.          # Loss count\n",
    "    log_interval = 200       # Set log interval to measure performance\n",
    "    start_time = time.time() # Start timer\n",
    "\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        # This deals with last batch, which may include smaller sized sequences\n",
    "        cor_seq_len = min(seq_len, batch.size()[1])\n",
    "\n",
    "        target_mask = generate_square_subsequent_mask(cor_seq_len-1).to(device) # Generate target mask\n",
    "        seq_mask = sequence_mask(cor_seq_len, batch.size(0), -1).to(device)     # Generate sequence mask\n",
    "        data    = batch[:, :-1]   # Set train data (for prediction)\n",
    "        targets = batch[:, 1: ]   # Set target data\n",
    "\n",
    "        target_mask = generate_square_subsequent_mask(cor_seq_len-1).to(device) # Generate target mask\n",
    "        seq_mask = sequence_mask(cor_seq_len, batch.size(0), -1).to(device)     # Generate sequence mask\n",
    "        \n",
    "        output  = model(batch, data, seq_mask, target_mask)\n",
    "        loss    = criterion(output.view(-1, output.size(-1)), targets.contiguous().view(-1))\n",
    "\n",
    "        # back propagation \n",
    "        if chosen_sched == 'SGD':                                                       \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)         # Prevent exploding gradients\n",
    "            optimizer.step()\n",
    "        \n",
    "        elif chosen_sched == 'Adam':  \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)         # Prevent exploding gradients\n",
    "            scheduler.step()\n",
    "            scheduler.optimizer.zero_grad()\n",
    "\n",
    "        # Count total loss\n",
    "        total_loss += loss.item()                                          \n",
    "        \n",
    "        # See performance throughout the model's training\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            # Try to get lr depends on the scheduler\n",
    "            try:\n",
    "                lr = scheduler.get_last_lr()[0]                             # read current learning rate\n",
    "            except:\n",
    "                lr = scheduler._rate                                        # read current learning rate\n",
    "\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval # calculate mean computation time for each batch [ms]\n",
    "            cur_loss     = total_loss / log_interval                        # mean loss for log_intervals batches\n",
    "            ppl          = math.exp(cur_loss)                               # perplexity\n",
    "\n",
    "            # Print performance \n",
    "            print(f'| epoch {epoch:3d} | {idx:5d}/{len(train_loader):5d} batches | '\n",
    "                  f'lr {lr:02.4f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f} |')\n",
    "            \n",
    "            total_loss = 0                                                  # reset loss counter\n",
    "            start_time = time.time()                                        # reset timer\n",
    "            \n",
    "            ppl_train_monitor.append([ppl, idx, epoch, lr])  \n",
    "            \n",
    "            #return ppl_train_monitor                                       # release '#' for debugging\n",
    "   \n",
    "    return ppl_train_monitor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, seq_len, criterion, device, mode=0):\n",
    "    '''\n",
    "    Input: \n",
    "        model               The transformer model\n",
    "        data_loader         Data loader of the evaluation sequences \n",
    "        seq_len             Maximum sequence length\n",
    "        Criterion           Loss criterion\n",
    "        device              computation device\n",
    "        mode                Mode: 0 - Evaluation of performance, 1 - Get model output.\n",
    "\n",
    "    Output:\n",
    "        outputs             List of model's training performance - include: ppl, batch number, epoch, learning rate\n",
    "        loss                Normalized total loss by the size of the data_loader\n",
    "    '''\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    outputs    = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(data_loader):\n",
    "            # The following section deals with prediction function, which is not batches based.   \n",
    "            if batch.ndim == 1:\n",
    "                cor_seq_len = min(seq_len, len(batch))\n",
    "                data        = batch[:-1]\n",
    "                targets     = batch[1: ]\n",
    "                target_mask = generate_square_subsequent_mask(cor_seq_len-1).to(device) # Generate target mask\n",
    "                seq_mask    = sequence_mask(cor_seq_len, batch.size(0), -1).to(device)  # Generate sequence mask\n",
    "                seq_mask    = seq_mask[0,:]\n",
    "            else:\n",
    "                cor_seq_len = min(seq_len, batch.size()[1])\n",
    "                data        = batch[:, :-1]\n",
    "                targets     = batch[:, 1: ]\n",
    "                target_mask = generate_square_subsequent_mask(cor_seq_len-1).to(device) # Generate target mask\n",
    "                seq_mask    = sequence_mask(cor_seq_len, batch.size(0), -1).to(device)  # Generate sequence mask\n",
    "            \n",
    "            output      = model(batch, data, seq_mask, target_mask)\n",
    "            total_loss += criterion(output.view(-1, output.size(-1)), targets.contiguous().view(-1)).item()\n",
    "            \n",
    "            if mode == 1:\n",
    "                outputs.append(output)\n",
    "    \n",
    "    if mode == 1:\n",
    "        outputs = torch.stack(outputs)\n",
    "        return outputs\n",
    "    \n",
    "    else:\n",
    "        return total_loss / (len(data_loader) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, input, dictionary, seq_len, criterion, device, top_k=2, explore_last=0):\n",
    "    '''\n",
    "    Input: \n",
    "        model                   The transformer model\n",
    "        input                   The input sequences the model will predict \n",
    "        dictionary              The corpus dictionary\n",
    "        seq_len                 The sequences length\n",
    "        top_k                   The top k probabilities. Useful to generate random sentences and not only the most likley next word\n",
    "        Criterion               Loss criterion\n",
    "        device                  Computation device\n",
    "        explore_last            Flag for exploring only last words or not (1 - engaged, 0 - disengaged)\n",
    "\n",
    "    Output:\n",
    "        generated_sentences     The genereated sequences\n",
    "        real_sentences          The real sequences\n",
    "    '''\n",
    "    model.eval()                # Set model to evaluation mode\n",
    "    generated_sentences = []    # Generated sequences list\n",
    "    real_sentences      = []    # Real sequences list\n",
    "\n",
    "    # Find the output of the model using the set imput\n",
    "    output = evaluate(model.to(device), \n",
    "                      input.to(device), \n",
    "                      seq_len, \n",
    "                      criterion, \n",
    "                      device, \n",
    "                      mode=1)\n",
    "\n",
    "    # If explore_last is engaged, the output will show only the k-predicted words in the end of a sequence\n",
    "    if explore_last == 1:\n",
    "        \n",
    "        generated_last_words = []\n",
    "        real_last_words      = []\n",
    "        \n",
    "        # Iterate over each sequence\n",
    "        for sequence in range(output.size(0)):\n",
    "            output_array    = output[sequence, -1, :].cpu().detach().numpy() # Convert output to a numpy array\n",
    "            word_indices    = output_array.argsort()[-top_k:][::-1]          # get the index of the k-most likely words\n",
    "    \n",
    "            # Find the k-most likely words\n",
    "            predicted_words = [list(dictionary.keys())[word_index] for word_index in word_indices]\n",
    "\n",
    "            generated_last_words.append(predicted_words)\n",
    "            real_last_words.append(list(dictionary.keys())[input[sequence, -1]])\n",
    "        return generated_last_words, real_last_words\n",
    "    \n",
    "    # If explore_last is disengaged, the output will give generated sentences and real sentences\n",
    "    else:\n",
    "        # Iterate over each sequence\n",
    "        for sequence in range(output.size(0)):\n",
    "\n",
    "            generated_words = []\n",
    "            real_words      = []\n",
    "            \n",
    "            # Iterate over each word\n",
    "            for word in range(output.size(1)):\n",
    "                output_array = output[sequence, word, :].cpu().detach().numpy() # Convert output to a numpy array\n",
    "                word_indices = output_array.argsort()[-top_k:][::-1]            # get the index of the k-most likely words\n",
    "                \n",
    "                # Find the k-most likely words\n",
    "                predicted_words = [list(dictionary.keys())[word_index] for word_index in word_indices]\n",
    "\n",
    "                # Generate randomization (choose one of the k-likely words, or just the most likely word)\n",
    "                if np.random.randint(2) == 1:\n",
    "                    chosen_word = predicted_words[np.random.randint(top_k)]\n",
    "                    generated_words.append(chosen_word)\n",
    "                else:\n",
    "                    generated_words.append(predicted_words[0])\n",
    "                \n",
    "                # Add the real sequence words\n",
    "                real_words.append(list(dictionary.keys())[input[sequence, word + 1]])\n",
    "            \n",
    "            # Stack generated and real sentences in lists\n",
    "            generated_sentences.append(generated_words)\n",
    "            real_sentences.append(real_words)\n",
    "        \n",
    "        return generated_sentences, real_sentences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model's dimensions\n",
    "d_model    = 256\n",
    "output_dim = d_model\n",
    "hidden_dim = 4 * d_model\n",
    "max_seq    = 36\n",
    "heads      = 8\n",
    "N          = 6\n",
    "batch_size = 32\n",
    "epochs     = 10\n",
    "\n",
    "# Set device for computation\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.empty_cache() \n",
    "\n",
    "# Datasets path and files\n",
    "dataset_path = os.path.join(Path(os.getcwd()), 'Dataset\\wikitext-2')\n",
    "files  = ('wiki.train.tokens', 'wiki.valid.tokens', 'wiki.test.tokens')\n",
    "\n",
    "# Load corpus from data (including dictionary and sequence splitting)\n",
    "train_corpus = Corpus_loader(dataset_path, files[0], max_seq)\n",
    "valid_corpus = Corpus_loader(dataset_path, files[1], max_seq, dictionary=train_corpus.dictionary)\n",
    "test_corpus  = Corpus_loader(dataset_path, files[2], max_seq, dictionary=train_corpus.dictionary)\n",
    "\n",
    "dictionary_size = len(train_corpus.dictionary) \n",
    "\n",
    "# Set loss function and choose optimizer\n",
    "criterion    = nn.CrossEntropyLoss()\n",
    "chosen_sched = 'Adam'\n",
    "\n",
    "if chosen_sched == 'SGD':\n",
    "    heads     = 4\n",
    "    N         = 2\n",
    "    # Define Transformer model\n",
    "    model     = Transformer(dictionary_size, \n",
    "                            d_model, \n",
    "                            output_dim, \n",
    "                            hidden_dim, \n",
    "                            max_seq, \n",
    "                            N, \n",
    "                            heads).to(device)\n",
    "                            \n",
    "    # Set optimizer\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=5.0)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.5)\n",
    "\n",
    "elif chosen_sched == 'Adam':\n",
    "    # Define Transformer model\n",
    "    model     = Transformer(dictionary_size, \n",
    "                            d_model, \n",
    "                            output_dim, \n",
    "                            hidden_dim, \n",
    "                            max_seq, \n",
    "                            N, \n",
    "                            heads).to(device)\n",
    "\n",
    "    # Set optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "    scheduler = NoamOpt(hidden_dim, factor=1, warmup=400, optimizer=optimizer)\n",
    "\n",
    "# Load data sequences into batches\n",
    "train_loader = DataLoader(train_corpus.sequences.to(device), batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_corpus.sequences.to(device), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## START TRAINING! ##\n",
    "log_interval      = 200       # Set log interval to measure performance\n",
    "best_val_loss     = float('inf')\n",
    "best_model        = None\n",
    "ppl_eval_monitor  = []\n",
    "ppl_train_monitor = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time() \n",
    "    \n",
    "    # Train and extract perplexity monitoring for each epoch, and adds it to general perplexity monitor. \n",
    "    ppl_monitor = train(model, \n",
    "                        train_loader, \n",
    "                        max_seq, \n",
    "                        epoch, \n",
    "                        scheduler, \n",
    "                        optimizer, \n",
    "                        criterion, \n",
    "                        device, \n",
    "                        chosen_sched)\n",
    "\n",
    "    ppl_train_monitor.append(ppl_monitor)\n",
    "    \n",
    "    # Evaluate the model in corrent epoch\n",
    "    val_loss = evaluate(model, \n",
    "                        valid_loader, \n",
    "                        max_seq, \n",
    "                        criterion, \n",
    "                        device)\n",
    "                        \n",
    "    val_ppl  = math.exp(val_loss)\n",
    "\n",
    "    # Extract perplexity monitor of the evaluation data to general monitor perplexity \n",
    "    ppl_eval_monitor.append([val_ppl,\n",
    "                             ppl_train_monitor[-1][-1][1],\n",
    "                             epoch,\n",
    "                             ppl_train_monitor[-1][-1][3]]) \n",
    "    \n",
    "    # Calculate run time for epoch\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    \n",
    "    # Print progress of validation data\n",
    "    print('-' * 94)\n",
    "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f} [sec] | '\n",
    "          f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}  | ')\n",
    "    print('-' * 94)\n",
    "\n",
    "    # Choose best model and save it\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)  \n",
    "    \n",
    "    # Update the  learning rate and empty cache\n",
    "    scheduler.step()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "### Add Saving Trained Model ###\n",
    "save_path = 'best_trained_model_w_nn_atten.pt'\n",
    "torch.save(best_model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train and validation perplexity\n",
    "ppl_train_monitor = torch.Tensor(ppl_train_monitor)\n",
    "ppl_eval_monitor  = torch.Tensor(ppl_eval_monitor)\n",
    "\n",
    "ppl_steps_train   = torch.flatten(ppl_train_monitor[:,:,0])\n",
    "idx_steps_train   = torch.flatten(ppl_train_monitor[:,:,1])\n",
    "epoch_steps_train = torch.flatten(ppl_train_monitor[:,:,2])\n",
    "lr_steps_train    = torch.flatten(ppl_train_monitor[:,:,3])\n",
    "\n",
    "ppl_steps_eval    = torch.flatten(ppl_eval_monitor[:,0])\n",
    "idx_steps_eval    = torch.flatten(ppl_eval_monitor[:,1])\n",
    "epoch_steps_eval  = torch.flatten(ppl_eval_monitor[:,2])\n",
    "lr_steps_eval     = torch.flatten(ppl_eval_monitor[:,3])\n",
    "\n",
    "\n",
    "plt.plot(idx_steps_train + epoch_steps_train*log_interval*10, ppl_steps_train, label='Train Perplexity')\n",
    "plt.plot(idx_steps_eval  + epoch_steps_eval*log_interval*10 , ppl_steps_eval , label='Validation Perplexity')\n",
    "plt.xlabel('batch number * Epoch * log interval')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.title('Perplexity over time: Train vs. Validation datasets')\n",
    "plt.legend()\n",
    "\n",
    "# Draw vertical lines at the epochs where the learning rate was decreased\n",
    "lr_change_indexes = [i for i,l in enumerate(lr_steps_train) if l != lr_steps_train[i-1]]\n",
    "for i in lr_change_indexes:\n",
    "    plt.axvline(x = (idx_steps_train[i] + epoch_steps_train[i]*log_interval*10), \n",
    "                color='r', \n",
    "                linestyle='--', \n",
    "                label='Validation Perplexity',\n",
    "                linewidth = 0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model\n",
    "save_path = 'best_trained_model_w_nn_atten.pt'\n",
    "new_model = Transformer(dictionary_size, \n",
    "                        d_model, \n",
    "                        output_dim, \n",
    "                        hidden_dim, \n",
    "                        max_seq, \n",
    "                        N, \n",
    "                        heads)\n",
    "\n",
    "new_model.load_state_dict(torch.load(save_path, map_location=torch.device(device)))\n",
    "\n",
    "# Set model for evaluation only\n",
    "new_model.eval()\n",
    "\n",
    "# Evaluate the model on test dataset\n",
    "test_loader   = DataLoader(test_corpus.sequences.to(device) , batch_size=batch_size, shuffle=True)\n",
    "start_test    = time.time() \n",
    "\n",
    "test_loss     = evaluate(new_model.to(device), \n",
    "                         test_loader, \n",
    "                         max_seq, \n",
    "                         criterion, \n",
    "                         device)\n",
    "\n",
    "elapsed_time  = time.time() - start_test\n",
    "test_ppl      = math.exp(test_loss)\n",
    "\n",
    "print('-' * 60)\n",
    "print(f'|   Test perplexity {test_ppl:8.2f}   |   Elapsed {elapsed_time:8.2f} [sec]  |')\n",
    "print('-' * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict from 3 random sequences in test dataset\n",
    "random_sequences = test_corpus.sequences[random.sample(range(test_corpus.sequences.size(0)), 3),:]\n",
    "\n",
    "generated_sentences, real_sentences = predict(new_model, \n",
    "                                              random_sequences, \n",
    "                                              train_corpus.dictionary, \n",
    "                                              max_seq, \n",
    "                                              criterion, \n",
    "                                              device, \n",
    "                                              top_k=1)\n",
    "\n",
    "for i in range(len(generated_sentences)):\n",
    "    print(f\"output of model - Sentence number: {i:3d}\\n================\")\n",
    "    print(' '.join(generated_sentences[i]))\n",
    "    print('\\n')\n",
    "    print(f\"Actual sequence - Sentence number: {i:3d}\\n================\")\n",
    "    print(' '.join(real_sentences[i]))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_words, real_last_words = predict(new_model, \n",
    "                                           random_sequences, \n",
    "                                           train_corpus.dictionary, \n",
    "                                           max_seq, \n",
    "                                           criterion, \n",
    "                                           device, \n",
    "                                           top_k=5, \n",
    "                                           explore_last=1)\n",
    "                                           \n",
    "for i in range(len(predicted_words)):\n",
    "    print(f\"output of model - Sentence number: {i:3d}\\n================\")\n",
    "    print(predicted_words[i])\n",
    "    print('\\n')\n",
    "    print(f\"Actual sequence - Sentence number: {i:3d}\\n================\")\n",
    "    print(real_last_words[i])\n",
    "    print('\\n')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRANSFER LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets path and files\n",
    "Pen_Tree_dataset_path = os.path.join(Path(os.getcwd()), 'Dataset\\PennTreeBank')\n",
    "files_Pen_Tree        = ('ptb.train.txt', 'ptb.valid.txt', 'ptb.test.txt')\n",
    "\n",
    "# Load corpus from data (including dictionary and sequence splitting)\n",
    "Pen_Tree_train_corpus = Corpus_loader(Pen_Tree_dataset_path, files_Pen_Tree[0], max_seq)\n",
    "Pen_Tree_valid_corpus = Corpus_loader(Pen_Tree_dataset_path, files_Pen_Tree[1], max_seq, Pen_Tree_train_corpus.dictionary)\n",
    "Pen_Tree_test_corpus  = Corpus_loader(Pen_Tree_dataset_path, files_Pen_Tree[2], max_seq, Pen_Tree_train_corpus.dictionary)\n",
    "\n",
    "dictionary_size_trans = len(Pen_Tree_train_corpus.dictionary) \n",
    "\n",
    "# Load data sequences into batches\n",
    "train_loader = DataLoader(Pen_Tree_train_corpus.sequences.to(device), batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(Pen_Tree_valid_corpus.sequences.to(device), batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(Pen_Tree_test_corpus.sequences.to(device) , batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_model = copy.deepcopy(new_model)\n",
    "# Freeze layers\n",
    "for params in modified_model.parameters():\n",
    "    params.requires_grad = False\n",
    "\n",
    "# Modify last layer\n",
    "modified_model.linear = nn.Linear(d_model, dictionary_size_trans)\n",
    "for params in modified_model.linear.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "# Set optimizer\n",
    "if chosen_sched == 'SGD':\n",
    "    # Set optimizer\n",
    "    optimizer = torch.optim.SGD(modified_model.parameters(), lr=5.0)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.5)\n",
    "\n",
    "else:\n",
    "    # Set optimizer\n",
    "    optimizer = torch.optim.Adam(modified_model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "    scheduler = NoamOpt(hidden_dim, factor=1, warmup=400, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## START TRAINING! ##\n",
    "log_interval      = 200       # Set log interval to measure performance\n",
    "best_val_loss     = float('inf')\n",
    "best_model        = None\n",
    "ppl_eval_monitor  = []\n",
    "ppl_train_monitor = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time() \n",
    "    \n",
    "    # Train and extract perplexity monitoring for each epoch, and adds it to general perplexity monitor. \n",
    "    ppl_monitor = train(modified_model.to(device), \n",
    "                        train_loader, \n",
    "                        max_seq, \n",
    "                        epoch, \n",
    "                        scheduler, \n",
    "                        optimizer, \n",
    "                        criterion, \n",
    "                        device, \n",
    "                        chosen_sched)\n",
    "\n",
    "    ppl_train_monitor.append(ppl_monitor)\n",
    "    \n",
    "    # Evaluate the model in corrent epoch\n",
    "    val_loss = evaluate(modified_model, \n",
    "                        valid_loader,\n",
    "                        max_seq, \n",
    "                        criterion, \n",
    "                        device)\n",
    "                        \n",
    "    val_ppl  = math.exp(val_loss)\n",
    "\n",
    "    # Extract perplexity monitor of the evaluation data to general monitor perplexity \n",
    "    ppl_eval_monitor.append([val_ppl,\n",
    "                             ppl_train_monitor[-1][-1][1],\n",
    "                             epoch,\n",
    "                             ppl_train_monitor[-1][-1][3]]) \n",
    "    \n",
    "    # Calculate run time for epoch\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    \n",
    "    # Print progress of validation data\n",
    "    print('-' * 94)\n",
    "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f} [sec] | '\n",
    "          f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}  | ')\n",
    "    print('-' * 94)\n",
    "\n",
    "    # Choose best model and save it\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model    = copy.deepcopy(modified_model)  \n",
    "\n",
    "    # Update the  learning rate and empty cache\n",
    "    scheduler.step()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "### Add Saving Trained Model ###\n",
    "save_path = 'best_trained_transfered_model_w_nn_atten.pt'\n",
    "torch.save(best_model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model\n",
    "save_path = 'best_trained_transfered_model_w_nn_atten.pt'\n",
    "test_the_modified_model = Transformer(dictionary_size, \n",
    "                                      d_model, \n",
    "                                      output_dim, \n",
    "                                      hidden_dim, \n",
    "                                      max_seq, \n",
    "                                      N, \n",
    "                                      heads)\n",
    "\n",
    "test_the_modified_model.linear = nn.Linear(d_model, dictionary_size_trans)\n",
    "\n",
    "test_the_modified_model.load_state_dict(torch.load(save_path, map_location=torch.device(device)))\n",
    "\n",
    "# Set model for evaluation only\n",
    "test_the_modified_model.eval()\n",
    "\n",
    "# Evaluate the model on test dataset\n",
    "start_test    = time.time() \n",
    "test_loss     = evaluate(test_the_modified_model.to(device), \n",
    "                         test_loader, \n",
    "                         max_seq, \n",
    "                         criterion, \n",
    "                         device)\n",
    "                         \n",
    "elapsed_time  = time.time() - start_test\n",
    "test_ppl      = math.exp(test_loss)\n",
    "\n",
    "print('-' * 60)\n",
    "print(f'|   Test perplexity {test_ppl:8.2f}   |   Elapsed {elapsed_time:8.2f} [sec]  |')\n",
    "print('-' * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict from 3 random sequences in test dataset\n",
    "random_sequences = Pen_Tree_test_corpus.sequences[random.sample(range(Pen_Tree_test_corpus.sequences.size(0)), 3),:]\n",
    "\n",
    "generated_sentences, real_sentences = predict(test_the_modified_model, \n",
    "                                              random_sequences, \n",
    "                                              Pen_Tree_train_corpus.dictionary, \n",
    "                                              max_seq, \n",
    "                                              criterion, \n",
    "                                              device, \n",
    "                                              top_k=1)\n",
    "\n",
    "for i in range(len(generated_sentences)):\n",
    "    print(f\"output of model - Sentence number: {i:3d}\\n================\")\n",
    "    print(' '.join(generated_sentences[i]))\n",
    "    print('\\n')\n",
    "    print(f\"Actual sequence - Sentence number: {i:3d}\\n================\")\n",
    "    print(' '.join(real_sentences[i]))\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:39:17) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "f7d800ec1281210af2e9ff328fbdec3c68c6a0a400632e06fb16360d57c70088"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
